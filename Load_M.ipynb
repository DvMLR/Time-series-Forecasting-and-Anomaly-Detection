{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Use GPU: cuda:0\n",
      "pred 1\n",
      "Cargar modelo del disco\n"
     ]
    }
   ],
   "source": [
    "from utils.tools import dotdict\n",
    "from exp.exp_informer import Exp_Informer\n",
    "import torch\n",
    "import numpy as np\n",
    "import os\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from keras.models import model_from_json\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "dataframe = pd.read_csv(\"DatosScaler.csv\")\n",
    "dataframe = dataframe.drop(['Unnamed: 0'], axis=1)\n",
    "datos = dataframe.values\n",
    "scaler = StandardScaler()\n",
    "scaler.fit(datos)\n",
    "\n",
    "\n",
    "args = dotdict()\n",
    "\n",
    "args.model = 'informer' # model of experiment, options: [informer, informerstack, informerlight(TBD)]\n",
    "\n",
    "args.data = 'custom' # data\n",
    "args.root_path = './data/ETT/' # root path of data file\n",
    "args.data_path = 'Datos.csv' # data file\n",
    "args.features = 'M' # forecasting task, options:[M, S, MS]; M:multivariate predict multivariate, S:univariate predict univariate, MS:multivariate predict univariate\n",
    "args.target = 'X54' # target feature in S or MS task\n",
    "args.freq = 'h' # freq for time features encoding, options:[s:secondly, t:minutely, h:hourly, d:daily, b:business days, w:weekly, m:monthly], you can also use more detailed freq like 15min or 3h\n",
    "args.checkpoints = './checkpoints' # location of model checkpoints\n",
    "\n",
    "args.seq_len = 96 # input sequence length of Informer encoder\n",
    "args.label_len = 48 # start token length of Informer decoder\n",
    "args.pred_len = 24 # prediction sequence length\n",
    "# Informer decoder input: concat[start token series(label_len), zero padding series(pred_len)]\n",
    "\n",
    "args.enc_in = 3 # encoder input size\n",
    "args.dec_in = 3 # decoder input size\n",
    "args.c_out = 3 # output size\n",
    "args.factor = 5 # probsparse attn factor\n",
    "args.d_model = 512 # dimension of model\n",
    "args.n_heads = 8 # num of heads\n",
    "args.e_layers = 2 # num of encoder layers\n",
    "args.d_layers = 1 # num of decoder layers\n",
    "args.d_ff = 2048 # dimension of fcn in model\n",
    "args.dropout = 0.05 # dropout\n",
    "args.attn = 'prob' # attention used in encoder, options:[prob, full]\n",
    "args.embed = 'timeF' # time features encoding, options:[timeF, fixed, learned]\n",
    "args.activation = 'gelu' # activation\n",
    "args.distil = True # whether to use distilling in encoder\n",
    "args.output_attention = False # whether to output attention in ecoder\n",
    "args.mix = True\n",
    "args.padding = 0\n",
    "args.freq = 'h'\n",
    "\n",
    "args.batch_size = 32\n",
    "args.learning_rate = 0.0001\n",
    "args.loss = 'mse'\n",
    "args.lradj = 'type1'\n",
    "args.use_amp = False # whether to use automatic mixed precision training\n",
    "\n",
    "args.num_workers = 0\n",
    "args.itr = 1\n",
    "args.train_epochs = 6\n",
    "args.patience = 3\n",
    "args.des = 'exp'\n",
    "\n",
    "args.use_gpu = True if torch.cuda.is_available() else False\n",
    "args.gpu = 0\n",
    "\n",
    "args.use_multi_gpu = False\n",
    "args.devices = '0,1,2,3'\n",
    "Exp = Exp_Informer  #se llama al modelo\n",
    "setting = 'informer_custom_ftM_sl96_ll48_pl24_dm512_nh8_el2_dl1_df2048_atprob_fc5_ebtimeF_dtTrue_mxTrue_test_0' #dirección dónde se encuentra guardado el mejor peso\n",
    "\n",
    "exp = Exp(args) # se crea experimenmto junto a los argumentos (argumentos entran al modelo)\n",
    "exp.predict(setting, True)\n",
    "pronóstico = np.load('./results/'+setting+'/real_prediction.npy') #se carga pronóstico desde archivo .npy\n",
    "scaler_class = scaler.transform(pronóstico)\n",
    "v = scaler_class.reshape((scaler_class.shape[0], scaler_class.shape[1], 1)) #se prepara la entrada para el siguiente modelo\n",
    "optimizer=keras.optimizers.Adam(learning_rate=1e-3)\n",
    "json_file = open('model_guardado5.json', 'r')\n",
    "loaded_model_json = json_file.read()\n",
    "json_file.close()\n",
    "loaded_model = model_from_json(loaded_model_json)\n",
    "# cargar los pesos del nuevo modelo\n",
    "loaded_model.load_weights(\"weights.best5.hdf5\")\n",
    "print(\"Cargar modelo del disco\")\n",
    "# evaluar el modelo cargado desde el disco con los datos \n",
    "loaded_model.compile(loss='sparse_categorical_crossentropy', optimizer=optimizer)\n",
    "clasificador = loaded_model.predict(v)\n",
    "folder_path = './results/' + setting +'/'\n",
    "np.save(folder_path+'real_class.npy', clasificador)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def informacion():\n",
    "    \n",
    "    global anomalias, anomalias_corrientes, anomalias_vibla, anomalias_vibll\n",
    "    pronóstico = np.load('./results/'+setting+'/real_prediction.npy') #se carga pronóstico desde archivo .npy\n",
    "    categoria=np.load('./results/'+setting+'/real_class.npy') #se carga pronóstico desde archivo .npy\n",
    "    pred_category = np.zeros(24)\n",
    "    i=0\n",
    "    while i<=23:\n",
    "        pred_category[i] = np.argmax(categoria[i])\n",
    "        i = i+1 \n",
    "\n",
    "    x1 = list(pronóstico[:, 0])\n",
    "    x2 = list(pronóstico[:, 1])\n",
    "    x3 = list(pronóstico[:, 2])\n",
    "    x4 = list(pred_category)\n",
    "    tabla=[x1]+[x2]+[x3]+[x4]\n",
    "    tabla=np.transpose(tabla)\n",
    "    columnas = ['Corriente','vibla','vibll','anomalias']\n",
    "    tabla = pd.DataFrame(tabla, columns=columnas)\n",
    "    anomalias= tabla.loc[tabla.anomalias == True]\n",
    "    anomalias_corrientes = tabla.loc[(tabla.Corriente > 274) | (tabla.Corriente < 253)]\n",
    "    anomalias_vibla = tabla.loc[(tabla.vibla > 2.85) | (tabla.vibla <  1.99)]\n",
    "    anomalias_vibll = tabla.loc[(tabla.vibll > 2.18) | (tabla.vibll < 1.2)]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_anomalias = len(anomalias_corrientes) + len(anomalias_vibla) + len(anomalias_vibll)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.13 ('models')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13 (default, Mar 28 2022, 06:59:08) [MSC v.1916 64 bit (AMD64)]"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "b4241c3b0a7969e332a74cc2801de4e53334df11b774d78a20e7811885e80442"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
